#!/usr/bin/env python3

"""Fetch files which have been added to the EBMS since the last run.

This script determines which files named in exported/files.json are
not already in our set of managed EBMS files, which are stored in
the files.tar archive file. The assumption is that all of the files
in that TAR file have a checksum recorded in the files.sums file.
The exported/files.json file is generated by the export.py script,
which pulls the information from the file_managed database table.
This script fetches the new files and writes them to the files
subdirectory.

Perform these steps each time this script is run.

  ./export.py
  rm -rf files
  mkdir files
  ./get-new-files.py
  tar -rf files.tar files
  rsync -a files ../web/sites/default/

After the final migration to the new production site, in order to confirm
that the files were transferred intact, you can perform the following steps:

  # On nciws-p2154-v using the drupal login account:
  NAME=`/bin/date +"/local/home/drupal/ebms-files-%Y%m%d.sums.gz"`
  cd /local/drupal/sites/ebms.nci.nih.gov
  /bin/find files -type f -exec /bin/sha1sum '{}' \; | /bin/gzip > $NAME

  # On the new production EBMS web server, to which the file created
  # on nciws-p2154-v has been copied:
  cd /local/drupal/ebms/migration
  /bin/gzip < FILE_COPIED_FROM_NCIWS-P2154-V | ./verify-file-checksums.py
"""

from datetime import datetime
from hashlib import sha1
from json import loads
from pathlib import Path
from urllib.parse import quote
from requests import get

HOST = "ebms.nci.nih.gov"
FILES = f"https://{HOST}/sites/ebms.nci.nih.gov/files"

# Collect the existing checksums.
start = datetime.now()
sums = {}
with open("files.sums", encoding="utf-8") as fp:
    for line in fp:
        checksum, path = line.strip().split(None, 1)
        sums[path] = checksum
original_count = len(sums)
print(f"loaded {original_count} existing checksums")

# Find out which new files we need to fetch.
with open("exported/files.json", encoding="utf-8") as fp:
    for line in fp:
        values = loads(line)
        fid = values["fid"]
        uri = values.get("uri")
        if uri and uri.startswith("public://"):
            filepath = uri.replace("public://", "")
            key = f"files/{filepath}"
            if key not in sums:
                url = f"{FILES}/{quote(filepath)}"
                response = get(url)
                content = response.content
                filesize = len(content)
                expected = values["filesize"]
                if filesize != expected:
                    err = (f"fid {fid}: expected {expected} bytes, "
                           "got {filesize}")
                    with open("fetch-files.err", "a", encoding="utf-8") as fp:
                        fp.write(f"{err} ({url})\n")
                    print(err)
                    continue
                print(f"{fid}: fetched {path}")
                path = Path(f"files/{filepath}")
                if len(path.parts) > 2:
                    directory = Path("/".join(path.parts[:-1]))
                    if not directory.exists():
                        directory.mkdir(parents=True)
                path.write_bytes(content)
                sums[key] = sha1(content).hexdigest()
added = len(sums) - original_count
if added:
    path = Path("files.sums")
    stamp = datetime.now().strftime("%Y%m%d%H%M%S")
    path.rename(f"files-{stamp}.sums")
    with open("files.sums", "w", encoding="utf-8") as fp:
        for path in sorted(sums):
            fp.write(f"{sums[path]} {path}\n")
    print(f"added {added} new files")
else:
    print("no new files found")
elapsed = datetime.now() - start
print(f"elapsed: {elapsed}")
